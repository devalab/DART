{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "# import pdb\n",
    "from torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DartModel(nn.Module):\n",
    "    def __init__(self, lh, lo, li):\n",
    "        super(DartModel, self).__init__()\n",
    "        self.fi1 = nn.Linear(3, lh)\n",
    "        self.fi2 = nn.Linear(lh, lo)\n",
    "        \n",
    "        self.fj1 = nn.Linear(3, lh)\n",
    "        self.fj2 = nn.Linear(lh, lo)\n",
    "\n",
    "        self.fk1 = nn.Linear(3, lh)\n",
    "        self.fk2 = nn.Linear(lh, lo)\n",
    "        \n",
    "        self.fl1 = nn.Linear(3, lh)\n",
    "        self.fl2 = nn.Linear(lh, lo)\n",
    "        \n",
    "        self.inter1 = nn.Linear(li, 256)\n",
    "        self.inter2 = nn.Linear(256, 128)\n",
    "        self.inter3 = nn.Linear(128, 32)\n",
    "        self.inter4 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, ai, aj, ak, al):\n",
    "        ai_sum = ai.sum(axis=2)\n",
    "        same_shape = ai_sum.shape\n",
    "        ones = torch.ones(same_shape, device=device)\n",
    "        zeros = torch.zeros(same_shape, device=device)\n",
    "        make_zero = torch.where(ai_sum==0, zeros, ones)\n",
    "        ai_mask = make_zero.unsqueeze(dim=2)\n",
    "\n",
    "        aj_sum = aj.sum(axis=3)\n",
    "        same_shape = aj_sum.shape\n",
    "        ones = torch.ones(same_shape, device=device)\n",
    "        zeros = torch.zeros(same_shape, device=device)\n",
    "        make_zero = torch.where(aj_sum==0, zeros, ones)\n",
    "        aj_mask = make_zero.unsqueeze(dim=3)\n",
    "\n",
    "        ak_sum = ak.sum(axis=3)\n",
    "        same_shape = ak_sum.shape\n",
    "        ones = torch.ones(same_shape, device=device)\n",
    "        zeros = torch.zeros(same_shape, device=device)\n",
    "        make_zero = torch.where(ak_sum==0, zeros, ones)\n",
    "        ak_mask = make_zero.unsqueeze(dim=3)\n",
    "\n",
    "        al_sum = al.sum(axis=3)\n",
    "        same_shape = al_sum.shape\n",
    "        ones = torch.ones(same_shape, device=device)\n",
    "        zeros = torch.zeros(same_shape, device=device)\n",
    "        make_zero = torch.where(al_sum==0, zeros, ones)\n",
    "        al_mask = make_zero.unsqueeze(dim=3)\n",
    "\n",
    "        ######### atom_i ############\n",
    "        ai = F.celu(self.fi1(ai), 0.1)\n",
    "        ai = F.celu(self.fi2(ai), 0.1)\n",
    "        ai = ai * ai_mask\n",
    "        ######### atom_j ############\n",
    "        aj = F.celu(self.fj1(aj), 0.1)\n",
    "        aj = F.celu(self.fj2(aj), 0.1)\n",
    "        aj = aj * aj_mask\n",
    "        ######### atom_k ############\n",
    "        ak = F.celu(self.fk1(ak), 0.1)\n",
    "        ak = F.celu(self.fk2(ak), 0.1)\n",
    "        ak = ak * ak_mask\n",
    "        ######## atom_l ############\n",
    "        al = F.celu(self.fl1(al), 0.1)\n",
    "        al = F.celu(self.fl2(al), 0.1)\n",
    "        al = al * al_mask\n",
    "\n",
    "        ########### interactions of i, j, k and l atoms ############\n",
    "        atm = ai + aj.sum(axis=2) + al.sum(axis=2) + al.sum(axis=2) # sum all interaction\n",
    "        atm = F.celu(self.inter1(atm), 0.1)\n",
    "        atm = F.celu(self.inter2(atm), 0.1)\n",
    "        atm = F.celu(self.inter3(atm), 0.1)\n",
    "        atm = self.inter4(atm)\n",
    "        atm = atm * ai_mask\n",
    "        return atm\n",
    "\n",
    "Dart_model = DartModel(128, 128, 128).to(device)\n",
    "\n",
    "def init_params(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=1.0)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "Dart_model.apply(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sep_ijkl_dataset(Dataset):\n",
    "    def __init__(self, file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.data = np.load(file, allow_pickle=True)\n",
    "        self.ener = self.data[\"ener\"]\n",
    "        batch = self.data[\"desc\"]\n",
    "        self.ener = torch.tensor([j for i in self.ener for j in i], dtype=torch.float, device=device)\n",
    "        batch_size = len(self.ener)\n",
    "        max_atoms = []\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            lol = batch[batch_idx]\n",
    "            max_atoms.append(len(lol[0])) # find longest sequence\n",
    "            max_atoms.append(max([len(i) for i in lol[1]])) # find longest sequence\n",
    "            max_atoms.append(max([len(i) for i in lol[2]])) # find longest sequence\n",
    "            max_atoms.append(max([len(i) for i in lol[3]])) # find longest sequence\n",
    "        iic = max_atoms[0::4]\n",
    "        jjc = max_atoms[1::4]\n",
    "        kkc = max_atoms[2::4]\n",
    "        llc = max_atoms[3::4]\n",
    "\n",
    "        des_j = []\n",
    "        des_k = []\n",
    "        des_l = []\n",
    "        for i in range(batch_size):\n",
    "            const_atom_count_i = max(iic) - iic[i]\n",
    "            const_atom_count_j = max(jjc) - jjc[i]\n",
    "            const_atom_count_k = max(kkc) - kkc[i]\n",
    "            const_atom_count_l = max(llc) - llc[i]\n",
    "            a_j = torch.zeros(const_atom_count_i, const_atom_count_j, 3)\n",
    "            a_k = torch.zeros(const_atom_count_i, const_atom_count_k, 3)\n",
    "            a_l = torch.zeros(const_atom_count_i, const_atom_count_l, 3)\n",
    "            des_j.append(pad_sequence([torch.tensor(i) for i in batch[i][1]] + [i for i in a_j]))\n",
    "            des_k.append(pad_sequence([torch.tensor(i) for i in batch[i][2]] + [i for i in a_k]))\n",
    "            des_l.append(pad_sequence([torch.tensor(i) for i in batch[i][3]] + [i for i in a_l]))\n",
    "        \n",
    "        self.des_i = pad_sequence([torch.tensor(batch[i][0]) for i in range(batch_size)], batch_first=True).squeeze().float().to(device)\n",
    "        des_j = pad_sequence(des_j, batch_first=True)\n",
    "        self.des_j = torch.transpose(des_j, 1, 2).float().to(device)\n",
    "\n",
    "        des_k = pad_sequence(des_k, batch_first=True)\n",
    "        self.des_k = torch.transpose(des_k, 1, 2).float().to(device)\n",
    "\n",
    "        des_l = pad_sequence(des_l, batch_first=True)\n",
    "        self.des_l = torch.transpose(des_l, 1, 2).float().to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ener)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\"atm_i\": self.des_i[idx], \"atm_j\": self.des_j[idx], \"atm_k\": self.des_k[idx], \"atm_l\": self.des_l[idx], \"energy\": self.ener[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train:Validation:Test split  and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_data = sep_ijkl_dataset(\"../data/small_dataset.npz\")\n",
    "\n",
    "validation_split = .1\n",
    "test_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(desc_data)\n",
    "indices = list(range(dataset_size))\n",
    "splitv = int(np.floor(validation_split * dataset_size))\n",
    "splitt = int(np.floor(test_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices, test_indices = indices[splitt+splitv:], indices[:splitv], indices[splitv:splitt+splitv]\n",
    "\n",
    "# Creating data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = DataLoader(desc_data, batch_size=32, sampler=train_sampler)\n",
    "validloader = DataLoader(desc_data, batch_size=32, sampler=valid_sampler)\n",
    "testloader = DataLoader(desc_data, batch_size=32, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(Dart_model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=25, verbose=True, eps=1e-09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochal_train_losses = []\n",
    "epochal_val_losses  = []\n",
    "num_epochs = 1500\n",
    "epoch_freq = 1\n",
    "       \n",
    "def test(Dart_model, testloader):\n",
    "    mae = torch.nn.L1Loss()\n",
    "    rmse = torch.nn.MSELoss()\n",
    "    pred_energy = torch.tensor([], device=\"cuda\")\n",
    "    real_energy = torch.tensor([], device=\"cuda\")\n",
    "    cluster_size = torch.tensor([], device=\"cuda\")\n",
    "    Dart_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            energy = Dart_model(batch[\"atm_i\"], batch[\"atm_j\"], batch[\"atm_k\"], batch[\"atm_l\"])\n",
    "            energy = energy.sum(axis=1).squeeze(dim=1)\n",
    "            pred_energy = torch.cat((pred_energy, energy))\n",
    "            real_energy = torch.cat((real_energy, batch[\"energy\"]))\n",
    "            cluster_size = torch.cat((cluster_size, batch[\"atm_i\"][:,0].sum(axis=1)))\n",
    "        results = torch.stack((cluster_size, real_energy, pred_energy), axis=1)\n",
    "        test_loss = mae(pred_energy, real_energy)\n",
    "        rmse_loss = torch.sqrt(rmse(pred_energy, real_energy))\n",
    "        print(\"Test MAE = \", test_loss.item(), \"Test RMSE = \", rmse_loss.item())\n",
    "        return results, test_loss, rmse_loss\n",
    "    \n",
    "def train(Dart_model, optimizer, epochal_train_losses, criterion):\n",
    "    train_loss = 0.00\n",
    "    n = 0\n",
    "    Dart_model.train()\n",
    "    for batch in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        energy = Dart_model(batch[\"atm_i\"], batch[\"atm_j\"], batch[\"atm_k\"], batch[\"atm_l\"])\n",
    "        energy = energy.sum(axis=1)\n",
    "        batch_loss = criterion(energy, batch[\"energy\"].unsqueeze(dim=1))\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += batch_loss.detach().cpu()\n",
    "        n += 1\n",
    "    train_loss /= n\n",
    "    epochal_train_losses.append(train_loss)\n",
    "\n",
    "def train_and_evaluate(Dart_model, optimizer, scheduler, criterion, start_epoch=1, restart=None):\n",
    "    if restart:\n",
    "        restore_path = os.path.join(log_dir + \"/last.pth.tar\")\n",
    "        checkpoint = load_checkpoint(restore_path, Dart_model, optimizer)\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    best_val = 100000.00\n",
    "    early_stopping_learning_rate = 1.0E-8\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        learning_rate = optimizer.param_groups[0]['lr']\n",
    "        if learning_rate < early_stopping_learning_rate:\n",
    "            break\n",
    "\n",
    "        ############ training #############\n",
    "        train(Dart_model, optimizer, epochal_train_losses, criterion)\n",
    "        \n",
    "        ############ validation #############\n",
    "        n=0\n",
    "        val_loss = 0.0\n",
    "        Dart_model.eval()\n",
    "        for batch in validloader:\n",
    "            energy = Dart_model(batch[\"atm_i\"], batch[\"atm_j\"], batch[\"atm_k\"], batch[\"atm_l\"])\n",
    "            energy = energy.sum(axis=1)\n",
    "            batch_loss = criterion(energy, batch[\"energy\"].unsqueeze(dim=1))\n",
    "            val_loss += batch_loss.detach().cpu()\n",
    "            n += 1\n",
    "        val_loss /= n\n",
    "        epochal_val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "     \n",
    "        is_best = val_loss <= best_val\n",
    "        if epoch % epoch_freq == 0:\n",
    "            print(\"Epoch: {: <5} Train: {: <20} Val: {: <20}\".format(epoch, epochal_train_losses[-1], val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(Dart_model, optimizer, scheduler, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on unseen data (test-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, test_mae, test_rmse = test(Dart_model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(0, len(epochal_train_losses[10:]), 1), epochal_train_losses[10:], label='Training Loss')\n",
    "# plt.plot(np.arange(0, len(epochal_train_losses[10:]), 1), epochal_val_losses[10:], label='validation loss')\n",
    "# plt.legend(loc='best')\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, len(epochal_train_losses), 1), epochal_train_losses, label='Training Loss')\n",
    "plt.plot(np.arange(0, len(epochal_train_losses), 1), epochal_val_losses, label='validation loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[results[:,0].argsort()].cpu().numpy()\n",
    "\n",
    "# plt.plot(np.arange(0, results.shape[0]), abs(results[:,1]-results[:,2]), label='Training Loss')\n",
    "plt.hist(abs(results[:,1]-results[:,2]), density=True)\n",
    "plt.xlabel(\"MAE\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_res = np.split(results[:,1:], np.unique(results[:, 0], return_index=True)[1][1:])\n",
    "diff = [abs(i[:,0]-i[:,1]).mean() for i in some_res]\n",
    "plt.title(\"Cluster size vs MAE\")\n",
    "plt.bar(np.arange(31, 41), diff, label='Training Loss')\n",
    "plt.xlabel(\"Cluster size\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = desc_data[train_indices][\"atm_i\"]\n",
    "lol = trainset[:,0].sum(axis=1)\n",
    "sizzle = [i.item() for i in lol]\n",
    "plt.title(\"Train set Cluster distribution, size = {}\".format(len(sizzle)))\n",
    "plt.xlabel(\"Cluster size\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.hist(sizzle, bins=10)\n",
    "plt.show()\n",
    "# plt.savefig(\"cluster_distribution_trainset.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = desc_data[test_indices][\"atm_i\"]\n",
    "lol = trainset[:,0].sum(axis=1)\n",
    "sizzle = [i.item() for i in lol]\n",
    "plt.title(\"Test set Cluster distribution, size = {}\".format(len(sizzle)))\n",
    "plt.xlabel(\"Cluster size\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.hist(sizzle, bins=39)\n",
    "plt.show()\n",
    "# plt.savefig(\"cluster_distribution_testset.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = desc_data[val_indices][\"atm_i\"]\n",
    "lol = trainset[:,0].sum(axis=1)\n",
    "sizzle = [i.item() for i in lol]\n",
    "plt.title(\"Validation set Cluster distribution, size = {}\".format(len(sizzle)))\n",
    "plt.hist(sizzle, bins=39)\n",
    "plt.xlabel(\"Cluster size\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.show()\n",
    "# plt.savefig(\"cluster_distribution_validationset.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qmml] *",
   "language": "python",
   "name": "conda-env-qmml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
